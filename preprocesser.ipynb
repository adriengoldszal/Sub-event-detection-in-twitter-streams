{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Loading embeddings took 88.21 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_start_time = time.time()\n",
    "print(f\"Loading embeddings...\")\n",
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "print(f\"Loading embeddings took {time.time() - embeddings_start_time :.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if (\n",
    "        not word_vectors\n",
    "    ):  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "def batch_preprocess_tweets(df, batch_size=1000):\n",
    "    \"\"\"Main preprocessing function with filtering and batching\n",
    "    Link here https://www.lix.polytechnique.fr/~nikolentzos/files/meladianos_ecir18\n",
    "\n",
    "        1) Removing retweets\n",
    "        2) Removing duplicates\n",
    "        3) Removing @ mentions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting tweet preprocessing...\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Initial data filtering\n",
    "    print(\"\\nFiltering tweets...\")\n",
    "    initial_count = len(processed_df)\n",
    "\n",
    "    # 1. Remove retweets\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.startswith(\"RT \", na=False)]\n",
    "    retweets_removed = initial_count - len(processed_df)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"Tweet\"])\n",
    "    duplicates_removed = initial_count - retweets_removed - len(processed_df)\n",
    "\n",
    "    # 3. Remove tweets with @-mentions\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.contains(\"@\", na=False)]\n",
    "    mentions_removed = (\n",
    "        initial_count - retweets_removed - duplicates_removed - len(processed_df)\n",
    "    )\n",
    "\n",
    "    # Print filtering statistics\n",
    "    print(f\"Removed {retweets_removed} retweets\")\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Removed {mentions_removed} tweets with @-mentions\")\n",
    "    print(f\"Remaining tweets: {len(processed_df)}\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(len(processed_df) / batch_size))\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    processed_tweets = []\n",
    "    with tqdm(total=len(processed_df), desc=\"Processing tweets\") as pbar:\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(processed_df))\n",
    "\n",
    "            # Get current batch\n",
    "            batch = processed_df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "            # Process batch\n",
    "            batch_results = [preprocess_text(tweet) for tweet in batch]\n",
    "            processed_tweets.extend(batch_results)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    # Add processed tweets to DataFrame\n",
    "    processed_df[\"Tweet\"] = processed_tweets\n",
    "\n",
    "    # Print timing statistics\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per tweet: {total_time/len(processed_df):.4f} seconds\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs standard text preprocessing tasks:\n",
    "    1. Tokenization\n",
    "    2. Stopword removal\n",
    "    3. Punctuation and special character removal\n",
    "    4. URL removal\n",
    "    5. Porter stemming\n",
    "\n",
    "    Args:\n",
    "        text: String containing the tweet text\n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Tokenization : better tokenization through word_tokenize by NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization is kept (porter stemming less precise)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 5056050\n",
      "Preprocessing tweets...\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 2619447 retweets\n",
      "Removed 120425 duplicates\n",
      "Removed 464706 tweets with @-mentions\n",
      "Remaining tweets: 1851472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 1851472/1851472 [09:27<00:00, 3260.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Total processing time: 569.96 seconds\n",
      "Average time per tweet: 0.0003 seconds\n",
      "Preprocessing took 570.00 seconds\n",
      "       ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "6    11_0       11         0          0  1404575400000   \n",
      "7    11_0       11         0          0  1404575400000   \n",
      "9    11_0       11         0          0  1404575400000   \n",
      "10   11_0       11         0          0  1404575400000   \n",
      "12   11_0       11         0          0  1404575400000   \n",
      "..    ...      ...       ...        ...            ...   \n",
      "789  11_0       11         0          0  1404575419000   \n",
      "792  11_0       11         0          0  1404575419000   \n",
      "794  11_0       11         0          0  1404575419000   \n",
      "795  11_0       11         0          0  1404575419000   \n",
      "797  11_0       11         0          0  1404575419000   \n",
      "\n",
      "                                                 Tweet  \n",
      "6    hope argentina lose would fun see belgium go f...  \n",
      "7    watch argentina v belgium th july live go link...  \n",
      "9    even though hate belgium beating u waffle damn...  \n",
      "10   lionel messi scored assisted goal world cup ar...  \n",
      "12   three new player argentina teamone calamitious...  \n",
      "..                                                 ...  \n",
      "789          argentina netherlands winning today leggo  \n",
      "792  belgium tonight want young brigade win go belg...  \n",
      "794  ready first kick tonight first game arg bel fo...  \n",
      "795          bel hazard scorer go belgium fifaworldcup  \n",
      "797  argentina belgium go penalty reckon going goal...  \n",
      "\n",
      "[300 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"challenge_data/train_tweets\"):\n",
    "    df = pd.read_csv(\"challenge_data/train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "print(f\"Preprocessing tweets...\")\n",
    "tweet_processing_start = time.time()\n",
    "df = batch_preprocess_tweets(df)\n",
    "print(f\"Preprocessing took {time.time() - tweet_processing_start:.2f} seconds\")\n",
    "print(df.head(300))\n",
    "df.to_csv(\"preprocessed_tweets.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped 1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"preprocessed_tweets.csv\")\n",
    "original_count = len(df)\n",
    "df = df.dropna() \n",
    "rows_dropped = original_count - len(df)\n",
    "print(f\"Rows dropped {rows_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tweet embeddings...\n",
      "Embedding computation took 35.33 seconds\n"
     ]
    }
   ],
   "source": [
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "print(f\"Computing tweet embeddings...\")\n",
    "embedding_start = time.time()\n",
    "tweet_vectors = np.vstack(\n",
    "    [get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df[\"Tweet\"]]\n",
    ")\n",
    "print(f\"Embedding computation took {time.time() - embedding_start:.2f} seconds\")\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = (\n",
    "    period_features.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_features.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=[\"EventType\", \"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "# We extract the labels of our training samples\n",
    "y = period_features[\"EventType\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Evaluating on a test set:\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:  0.7398753894080997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 200)\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "195    0\n",
      "196    0\n",
      "197    0\n",
      "198    0\n",
      "199    0\n",
      "Length: 200, dtype: int64\n",
      "(130, 200)\n",
      "(130, 200)\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "195    0\n",
      "196    0\n",
      "197    0\n",
      "198    0\n",
      "199    0\n",
      "Length: 200, dtype: int64\n",
      "(130, 200)\n",
      "(130, 200)\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "195    0\n",
      "196    0\n",
      "197    0\n",
      "198    0\n",
      "199    0\n",
      "Length: 200, dtype: int64\n",
      "(130, 200)\n",
      "(126, 200)\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "195    0\n",
      "196    0\n",
      "197    0\n",
      "198    0\n",
      "199    0\n",
      "Length: 200, dtype: int64\n",
      "(126, 200)\n"
     ]
    }
   ],
   "source": [
    "# We set up a basic classifier that we train and then calculate the accuracy on our test set\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "###### For Kaggle submission\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n",
    "# We add a dummy classifier for sanity purposes\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"challenge_data/eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"challenge_data/eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "    tweet_vectors_test = np.vstack(\n",
    "        [\n",
    "            get_avg_embedding(tweet, embeddings_model, vector_size)\n",
    "            for tweet in val_df[\"Tweet\"]\n",
    "        ]\n",
    "    )\n",
    "    tweet_df_test = pd.DataFrame(tweet_vectors_test)\n",
    "\n",
    "    period_features_test = pd.concat([val_df, tweet_df_test], axis=1)\n",
    "    period_features_test = period_features_test.dropna() \n",
    "    period_features_test = period_features_test.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "    \n",
    "    period_features_test = (\n",
    "        period_features_test.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    "    )\n",
    "    X = period_features_test.drop(columns=[\"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "    X = pd.DataFrame(X)\n",
    "    print(X.shape)\n",
    "    print(X.isna().sum())\n",
    "    X = X.dropna()\n",
    "    print(X.shape)\n",
    "    \n",
    "    preds = clf.predict(X)\n",
    "    dummy_preds = dummy_clf.predict(X)\n",
    "\n",
    "    period_features_test[\"EventType\"] = preds\n",
    "    period_features_test[\"DummyEventType\"] = dummy_preds\n",
    "\n",
    "    predictions.append(period_features_test[[\"ID\", \"EventType\"]])\n",
    "    dummy_predictions.append(period_features_test[[\"ID\", \"DummyEventType\"]])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv(\"logistic_better_preprocessing_predictions.csv\", index=False)\n",
    "\n",
    "pred_df = pd.concat(dummy_predictions)\n",
    "pred_df.to_csv(\"dummy_better_preprocessing_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
