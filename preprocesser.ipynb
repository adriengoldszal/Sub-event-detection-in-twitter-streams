{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Loading embeddings took 213.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_start_time = time.time()\n",
    "print(f\"Loading embeddings...\")\n",
    "embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings\n",
    "print(f\"Loading embeddings took {time.time() - embeddings_start_time :.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if (\n",
    "        not word_vectors\n",
    "    ):  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "def batch_preprocess_tweets(df, batch_size=1000):\n",
    "    \"\"\"Main preprocessing function with filtering and batching\n",
    "    Link here https://www.lix.polytechnique.fr/~nikolentzos/files/meladianos_ecir18\n",
    "\n",
    "        1) Removing retweets\n",
    "        2) Removing duplicates\n",
    "        3) Removing @ mentions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting tweet preprocessing...\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Initial data filtering\n",
    "    print(\"\\nFiltering tweets...\")\n",
    "    initial_count = len(processed_df)\n",
    "\n",
    "    # 1. Remove retweets\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.startswith(\"RT \", na=False)]\n",
    "    retweets_removed = initial_count - len(processed_df)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"Tweet\"])\n",
    "    duplicates_removed = initial_count - retweets_removed - len(processed_df)\n",
    "\n",
    "    # 3. Remove tweets with @-mentions\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.contains(\"@\", na=False)]\n",
    "    mentions_removed = (\n",
    "        initial_count - retweets_removed - duplicates_removed - len(processed_df)\n",
    "    )\n",
    "\n",
    "    # Print filtering statistics\n",
    "    print(f\"Removed {retweets_removed} retweets\")\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Removed {mentions_removed} tweets with @-mentions\")\n",
    "    print(f\"Remaining tweets: {len(processed_df)}\")\n",
    "\n",
    "    # Handle any remaining NaN values\n",
    "    processed_df[\"Tweet\"] = processed_df[\"Tweet\"].fillna(\"\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(len(processed_df) / batch_size))\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    processed_tweets = []\n",
    "    with tqdm(total=len(processed_df), desc=\"Processing tweets\") as pbar:\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(processed_df))\n",
    "\n",
    "            # Get current batch\n",
    "            batch = processed_df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "            # Process batch\n",
    "            batch_results = [preprocess_text(tweet) for tweet in batch]\n",
    "            processed_tweets.extend(batch_results)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    # Add processed tweets to DataFrame\n",
    "    processed_df[\"Tweet\"] = processed_tweets\n",
    "\n",
    "    # Print timing statistics\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per tweet: {total_time/len(processed_df):.4f} seconds\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs standard text preprocessing tasks:\n",
    "    1. Tokenization\n",
    "    2. Stopword removal\n",
    "    3. Punctuation and special character removal\n",
    "    4. URL removal\n",
    "    5. Porter stemming\n",
    "\n",
    "    Args:\n",
    "        text: String containing the tweet text\n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Tokenization : better tokenization through word_tokenize by NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization is kept (porter stemming less precise)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 5056050\n",
      "Preprocessing tweets...\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 2619447 retweets\n",
      "Removed 120425 duplicates\n",
      "Removed 464706 tweets with @-mentions\n",
      "Remaining tweets: 1851472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets:  95%|█████████▍| 1758000/1851472 [12:11<00:34, 2727.41it/s] "
     ]
    }
   ],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"challenge_data/train_tweets\"):\n",
    "    df = pd.read_csv(\"challenge_data/train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "print(f\"Preprocessing tweets...\")\n",
    "tweet_processing_start = time.time()\n",
    "df = batch_preprocess_tweets(df)\n",
    "print(f\"Preprocessing took {time.time() - tweet_processing_start:.2f} seconds\")\n",
    "print(df.head(300))\n",
    "df.to_csv(\"preprocessed_tweets.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID           0\n",
      "MatchID      0\n",
      "PeriodID     0\n",
      "EventType    0\n",
      "Timestamp    0\n",
      "Tweet        1\n",
      "dtype: int64\n",
      "Computing tweet embeddings...\n",
      "Embedding computation took 109.91 seconds\n",
      "          0         1         2         3         4         5         6    \\\n",
      "0    0.143873  0.381260 -0.011760 -0.024444 -0.198438  0.175785  0.517592   \n",
      "1    0.058647  0.323562  0.093966 -0.206013 -0.152912 -0.008612  0.060596   \n",
      "2    0.069766  0.216052 -0.032773  0.306726 -0.307166  0.290943  0.275279   \n",
      "3    0.059672  0.179357  0.183643 -0.080278  0.410679  0.015973  0.084996   \n",
      "4    0.290052  0.201452  0.007394 -0.075158  0.012593 -0.179910  0.242341   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "295  0.384157  0.153585  0.173762 -0.239806 -0.128697  0.036120 -0.023490   \n",
      "296 -0.006440  0.177838 -0.159907 -0.071148 -0.316127  0.182168  0.161426   \n",
      "297 -0.003997  0.205275  0.133962 -0.375018 -0.014257  0.148191  0.174167   \n",
      "298  0.061636 -0.053289 -0.165514 -0.061262 -0.059658  0.099598 -0.221240   \n",
      "299  0.097780  0.106118  0.081896 -0.248042 -0.112715  0.218919  0.343339   \n",
      "\n",
      "          7         8         9    ...       190       191       192  \\\n",
      "0   -0.050235  0.236715 -0.038112  ... -0.211186  0.015064 -0.062502   \n",
      "1   -0.085975  0.252217 -0.062302  ...  0.020342 -0.137209  0.084570   \n",
      "2    0.026632 -0.043275 -0.129147  ... -0.246385 -0.061596  0.136495   \n",
      "3    0.013764  0.195465 -0.193067  ... -0.513905 -0.129282  0.095638   \n",
      "4   -0.028570 -0.000968 -0.379603  ... -0.194178  0.023315  0.026009   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "295  0.082477  0.242539 -0.342476  ... -0.121468 -0.198203  0.120760   \n",
      "296  0.111339  0.366743 -0.134670  ... -0.075142 -0.155511 -0.097171   \n",
      "297  0.058857  0.170774 -0.030526  ... -0.248450  0.061806  0.051145   \n",
      "298 -0.057161  0.201027 -0.202052  ... -0.377524 -0.079930  0.181037   \n",
      "299 -0.040329  0.219144 -0.123151  ... -0.178055 -0.154833  0.004607   \n",
      "\n",
      "          193       194       195       196       197       198       199  \n",
      "0   -0.191706  0.060244 -0.049812  0.359155 -0.094736 -0.034703  0.137832  \n",
      "1   -0.222285 -0.099865 -0.069974  0.157885 -0.012722 -0.126869  0.115961  \n",
      "2   -0.055652  0.003899 -0.187336  0.143424  0.061390 -0.121184  0.291085  \n",
      "3   -0.157810 -0.052194 -0.164326  0.296218 -0.113199 -0.012024  0.332755  \n",
      "4    0.248900 -0.207419 -0.128462  0.094133 -0.103522  0.008067  0.121786  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "295 -0.041027 -0.157002  0.055451  0.231435 -0.091009 -0.007805  0.475920  \n",
      "296 -0.158567 -0.027322 -0.098902  0.450401 -0.092591 -0.098689  0.144116  \n",
      "297  0.034974 -0.110268 -0.089191 -0.078104  0.110554  0.025704  0.372081  \n",
      "298 -0.122828  0.102306 -0.118905  0.214980  0.053310 -0.020919  0.374538  \n",
      "299 -0.078563 -0.011679 -0.081790  0.385801 -0.055027 -0.223741  0.425750  \n",
      "\n",
      "[300 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"preprocessed_tweets.csv\")\n",
    "print(df.isna().sum())\n",
    "df = df.dropna() \n",
    "\n",
    "\n",
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "print(f\"Computing tweet embeddings...\")\n",
    "embedding_start = time.time()\n",
    "tweet_vectors = np.vstack(\n",
    "    [get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df[\"Tweet\"]]\n",
    ")\n",
    "print(f\"Embedding computation took {time.time() - embedding_start:.2f} seconds\")\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "print(tweet_df.head(300))\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = (\n",
    "    period_features.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'period_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We drop the non-numerical features and keep the embeddings values for each period\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mperiod_features\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEventType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatchID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeriodID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# We extract the labels of our training samples\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m period_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEventType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'period_features' is not defined"
     ]
    }
   ],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=[\"EventType\", \"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "# We extract the labels of our training samples\n",
    "y = period_features[\"EventType\"].values\n",
    "\n",
    "###### Evaluating on a test set:\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# We set up a basic classifier that we train and then calculate the accuracy on our test set\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "###### For Kaggle submission\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n",
    "# We add a dummy classifier for sanity purposes\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"challenge_data/eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"challenge_data/eval_tweets/\" + fname)\n",
    "    val_df = batch_preprocess_tweets(val_df)\n",
    "    val_df.head(300)\n",
    "\n",
    "    tweet_vectors = np.vstack(\n",
    "        [\n",
    "            get_avg_embedding(tweet, embeddings_model, vector_size)\n",
    "            for tweet in val_df[\"Tweet\"]\n",
    "        ]\n",
    "    )\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features = period_features.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "    period_features = (\n",
    "        period_features.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    "    )\n",
    "    X = period_features.drop(columns=[\"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "\n",
    "    preds = clf.predict(X)\n",
    "    dummy_preds = dummy_clf.predict(X)\n",
    "\n",
    "    period_features[\"EventType\"] = preds\n",
    "    period_features[\"DummyEventType\"] = dummy_preds\n",
    "\n",
    "    predictions.append(period_features[[\"ID\", \"EventType\"]])\n",
    "    dummy_predictions.append(period_features[[\"ID\", \"DummyEventType\"]])\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv(\"logistic_better_preprocessing_predictions.csv\", index=False)\n",
    "\n",
    "pred_df = pd.concat(dummy_predictions)\n",
    "pred_df.to_csv(\"dummy_better_preprocessing_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
