{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Loading embeddings took 102.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_start_time = time.time()\n",
    "print(f\"Loading embeddings...\")\n",
    "embeddings_model = api.load(\"glove-twitter-200\") \n",
    "print(f\"Loading embeddings took {time.time() - embeddings_start_time :.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if (\n",
    "        not word_vectors\n",
    "    ):  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "def batch_preprocess_tweets(df, batch_size=1000):\n",
    "    \"\"\"Main preprocessing function with filtering and batching\n",
    "    Link here https://www.lix.polytechnique.fr/~nikolentzos/files/meladianos_ecir18\n",
    "\n",
    "        1) Removing retweets\n",
    "        2) Removing duplicates\n",
    "        3) Removing @ mentions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting tweet preprocessing...\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Initial data filtering\n",
    "    print(\"\\nFiltering tweets...\")\n",
    "    initial_count = len(processed_df)\n",
    "\n",
    "    # 1. Remove retweets\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.startswith(\"RT \", na=False)]\n",
    "    retweets_removed = initial_count - len(processed_df)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"Tweet\"])\n",
    "    duplicates_removed = initial_count - retweets_removed - len(processed_df)\n",
    "\n",
    "    # 3. Remove tweets with @-mentions\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.contains(\"@\", na=False)]\n",
    "    mentions_removed = (\n",
    "        initial_count - retweets_removed - duplicates_removed - len(processed_df)\n",
    "    )\n",
    "\n",
    "    # Print filtering statistics\n",
    "    print(f\"Removed {retweets_removed} retweets\")\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Removed {mentions_removed} tweets with @-mentions\")\n",
    "    print(f\"Remaining tweets: {len(processed_df)}\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(len(processed_df) / batch_size))\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    processed_tweets = []\n",
    "    with tqdm(total=len(processed_df), desc=\"Processing tweets\") as pbar:\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(processed_df))\n",
    "\n",
    "            # Get current batch\n",
    "            batch = processed_df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "            # Process batch\n",
    "            batch_results = [preprocess_text(tweet) for tweet in batch]\n",
    "            processed_tweets.extend(batch_results)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    # Add processed tweets to DataFrame\n",
    "    processed_df[\"Tweet\"] = processed_tweets\n",
    "\n",
    "    # Print timing statistics\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per tweet: {total_time/len(processed_df):.4f} seconds\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs standard text preprocessing tasks:\n",
    "    1. Tokenization\n",
    "    2. Stopword removal\n",
    "    3. Punctuation and special character removal\n",
    "    4. URL removal\n",
    "    5. Porter stemming\n",
    "\n",
    "    Args:\n",
    "        text: String containing the tweet text\n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Tokenization : better tokenization through word_tokenize by NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization is kept (porter stemming less precise)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 5056050\n",
      "Preprocessing tweets...\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 2619447 retweets\n",
      "Removed 120425 duplicates\n",
      "Removed 464706 tweets with @-mentions\n",
      "Remaining tweets: 1851472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 1851472/1851472 [10:32<00:00, 2927.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Total processing time: 635.29 seconds\n",
      "Average time per tweet: 0.0003 seconds\n",
      "Preprocessing took 635.36 seconds\n",
      "       ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "6    11_0       11         0          0  1404575400000   \n",
      "7    11_0       11         0          0  1404575400000   \n",
      "9    11_0       11         0          0  1404575400000   \n",
      "10   11_0       11         0          0  1404575400000   \n",
      "12   11_0       11         0          0  1404575400000   \n",
      "..    ...      ...       ...        ...            ...   \n",
      "789  11_0       11         0          0  1404575419000   \n",
      "792  11_0       11         0          0  1404575419000   \n",
      "794  11_0       11         0          0  1404575419000   \n",
      "795  11_0       11         0          0  1404575419000   \n",
      "797  11_0       11         0          0  1404575419000   \n",
      "\n",
      "                                                 Tweet  \n",
      "6    hope argentina lose would fun see belgium go f...  \n",
      "7    watch argentina v belgium th july live go link...  \n",
      "9    even though hate belgium beating u waffle damn...  \n",
      "10   lionel messi scored assisted goal world cup ar...  \n",
      "12   three new player argentina teamone calamitious...  \n",
      "..                                                 ...  \n",
      "789          argentina netherlands winning today leggo  \n",
      "792  belgium tonight want young brigade win go belg...  \n",
      "794  ready first kick tonight first game arg bel fo...  \n",
      "795          bel hazard scorer go belgium fifaworldcup  \n",
      "797  argentina belgium go penalty reckon going goal...  \n",
      "\n",
      "[300 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"challenge_data/train_tweets\"):\n",
    "    df = pd.read_csv(\"challenge_data/train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "print(f\"Preprocessing tweets...\")\n",
    "tweet_processing_start = time.time()\n",
    "df = batch_preprocess_tweets(df)\n",
    "print(f\"Preprocessing took {time.time() - tweet_processing_start:.2f} seconds\")\n",
    "print(df.head(300))\n",
    "df.to_csv(\"preprocessed_tweets_.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>PeriodID</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>hope argentina lose would fun see belgium go f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>watch argentina v belgium th july live go link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>even though hate belgium beating u waffle damn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>lionel messi scored assisted goal world cup ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>three new player argentina teamone calamitious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851467</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>sound like quite match morning bad know work f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851468</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>ok england tie score u advance right worldcup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851469</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>real soccer match many boring game worldcup us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851470</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>woah awesome game soccer played worldcup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851471</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>ugh shouldve usa worldcup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1851471 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "0          11_0       11         0          0  1404575400000   \n",
       "1          11_0       11         0          0  1404575400000   \n",
       "2          11_0       11         0          0  1404575400000   \n",
       "3          11_0       11         0          0  1404575400000   \n",
       "4          11_0       11         0          0  1404575400000   \n",
       "...         ...      ...       ...        ...            ...   \n",
       "1851467  18_129       18       129          0  1276876799000   \n",
       "1851468  18_129       18       129          0  1276876799000   \n",
       "1851469  18_129       18       129          0  1276876799000   \n",
       "1851470  18_129       18       129          0  1276876799000   \n",
       "1851471  18_129       18       129          0  1276876799000   \n",
       "\n",
       "                                                     Tweet  \n",
       "0        hope argentina lose would fun see belgium go f...  \n",
       "1        watch argentina v belgium th july live go link...  \n",
       "2        even though hate belgium beating u waffle damn...  \n",
       "3        lionel messi scored assisted goal world cup ar...  \n",
       "4        three new player argentina teamone calamitious...  \n",
       "...                                                    ...  \n",
       "1851467  sound like quite match morning bad know work f...  \n",
       "1851468      ok england tie score u advance right worldcup  \n",
       "1851469  real soccer match many boring game worldcup us...  \n",
       "1851470           woah awesome game soccer played worldcup  \n",
       "1851471                          ugh shouldve usa worldcup  \n",
       "\n",
       "[1851471 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"preprocessed_tweets.csv\")\n",
    "original_count = len(df)\n",
    "df = df.dropna() \n",
    "rows_dropped = original_count - len(df)\n",
    "print(f\"Rows dropped {rows_dropped}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tweet embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing tweet embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m embedding_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m tweet_vectors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(\n\u001b[0;32m----> 5\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mget_avg_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTweet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding computation took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39membedding_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m tweet_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tweet_vectors)\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing tweet embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m embedding_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m tweet_vectors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(\n\u001b[0;32m----> 5\u001b[0m     [\u001b[43mget_avg_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding computation took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39membedding_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m tweet_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tweet_vectors)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mget_avg_embedding\u001b[0;34m(tweet, model, vector_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m word_vectors\n\u001b[1;32m      7\u001b[0m ):  \u001b[38;5;66;03m# If no words in the tweet are in the vocabulary, return a zero vector\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(vector_size)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3385\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3381\u001b[0m                      where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[0;32m-> 3385\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_mean_dispatcher)\n\u001b[1;32m   3386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3387\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   3388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3389\u001b[0m \u001b[38;5;124;03m    Compute the arithmetic mean along the specified axis.\u001b[39;00m\n\u001b[1;32m   3390\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3489\u001b[0m \n\u001b[1;32m   3490\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3491\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "print(f\"Computing tweet embeddings...\")\n",
    "embedding_start = time.time()\n",
    "tweet_vectors = np.vstack(\n",
    "    [get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df[\"Tweet\"]]\n",
    ")\n",
    "print(f\"Embedding computation took {time.time() - embedding_start:.2f} seconds\")\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = (\n",
    "    period_features.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_features.isna().sum().sum()\n",
    "period_features.to_csv(\"period_features_glove.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_features = pd.read_csv(\"period_features_glove.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 171901 retweets\n",
      "Removed 3663 duplicates\n",
      "Removed 19888 tweets with @-mentions\n",
      "Remaining tweets: 90352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 90352/90352 [00:34<00:00, 2583.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Total processing time: 35.11 seconds\n",
      "Average time per tweet: 0.0004 seconds\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 5897 retweets\n",
      "Removed 762 duplicates\n",
      "Removed 5492 tweets with @-mentions\n",
      "Remaining tweets: 32873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 32873/32873 [00:12<00:00, 2536.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Total processing time: 12.99 seconds\n",
      "Average time per tweet: 0.0004 seconds\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 43813 retweets\n",
      "Removed 2708 duplicates\n",
      "Removed 8636 tweets with @-mentions\n",
      "Remaining tweets: 58245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 58245/58245 [00:22<00:00, 2577.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Total processing time: 22.66 seconds\n",
      "Average time per tweet: 0.0004 seconds\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 312330 retweets\n",
      "Removed 9101 duplicates\n",
      "Removed 58007 tweets with @-mentions\n",
      "Remaining tweets: 249260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 249260/249260 [01:35<00:00, 2615.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Total processing time: 95.61 seconds\n",
      "Average time per tweet: 0.0004 seconds\n",
      "Embedding computation took 12.04 seconds\n"
     ]
    }
   ],
   "source": [
    "vector_size = 200  # Adjust based on the chosen GloVe model\n",
    "# Concatenate all evaluation data first\n",
    "eval_dfs = []\n",
    "\n",
    "for fname in os.listdir(\"challenge_data/eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"challenge_data/eval_tweets/\" + fname)\n",
    "    val_df = batch_preprocess_tweets(val_df)\n",
    "    eval_dfs.append(val_df)\n",
    "\n",
    "# Combine all DataFrames first\n",
    "eval_df = pd.concat(eval_dfs, ignore_index=True)\n",
    "\n",
    "# Now process the combined DataFrame\n",
    "embedding_start = time.time()\n",
    "\n",
    "tweet_vectors = np.vstack(\n",
    "    [get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in eval_df[\"Tweet\"]]\n",
    ")\n",
    "\n",
    "tweet_df_test = pd.DataFrame(tweet_vectors)\n",
    "period_features_test = pd.concat([eval_df, tweet_df_test], axis=1)\n",
    "\n",
    "# Save the full DataFrame with embedded tweets\n",
    "period_features_test = period_features_test.drop(columns=[\"Tweet\"])\n",
    "period_features_test = (\n",
    "    period_features_test.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Embedding computation took {time.time() - embedding_start:.2f} seconds\")\n",
    "period_features_test.to_csv(\"tweets_with_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
