{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m embeddings_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m embeddings_model \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglove-twitter-50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 50-dimensional GloVe embeddings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading embeddings took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39membeddings_start_time\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/gensim/downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, BASE_DIR)\n\u001b[1;32m    502\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(name)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gensim-data/glove-twitter-50/__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[1;32m      7\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove-twitter-50\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove-twitter-50.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m         )\n\u001b[1;32m   2068\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2069\u001b[0m         \u001b[43m_word2vec_read_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv):\n\u001b[1;32m   2071\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2073\u001b[0m         kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(kv),\n\u001b[1;32m   2074\u001b[0m     )\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1975\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1974\u001b[0m word, weights \u001b[38;5;241m=\u001b[39m _word2vec_line_to_vector(line, datatype, unicode_errors, encoding)\n\u001b[0;32m-> 1975\u001b[0m \u001b[43m_add_word_to_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1923\u001b[0m, in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary file is incomplete: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is missing\u001b[39m\u001b[38;5;124m\"\u001b[39m, word)\n\u001b[1;32m   1922\u001b[0m     word_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1923\u001b[0m \u001b[43mkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_vecattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/gensim/models/keyedvectors.py:353\u001b[0m, in \u001b[0;36mKeyedVectors.set_vecattr\u001b[0;34m(self, key, attr, val)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_vecattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, attr, val):\n\u001b[1;32m    335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set attribute associated with the given key to value.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocate_vecattrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[attr][index] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/gensim/models/keyedvectors.py:323\u001b[0m, in \u001b[0;36mKeyedVectors.allocate_vecattrs\u001b[0;34m(self, attrs, types)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m prev_expando \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[attr]\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missubdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_expando\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt allocate type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflicts with its existing type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_expando\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prev_expando) \u001b[38;5;241m==\u001b[39m target_size:\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/numpy/core/numerictypes.py:416\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missubdtype\u001b[39m(arg1, arg2):\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    Returns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43missubclass_\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneric\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    417\u001b[0m         arg1 \u001b[38;5;241m=\u001b[39m dtype(arg1)\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg2, generic):\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/numpy/core/numerictypes.py:283\u001b[0m, in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missubclass_\u001b[39m(arg1, arg2):\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    Determine if a class is a subclass of a second class.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m \n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "# Load GloVe model with Gensim's API\n",
    "embeddings_start_time = time.time()\n",
    "print(f\"Loading embeddings...\")\n",
    "embeddings_model = api.load(\"glove-twitter-50\")  # 50-dimensional GloVe embeddings\n",
    "print(f\"Loading embeddings took {time.time() - embeddings_start_time :.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=50):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if (\n",
    "        not word_vectors\n",
    "    ):  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "def batch_preprocess_tweets(df, batch_size=1000):\n",
    "    \"\"\"Main preprocessing function with filtering and batching\n",
    "    Link here https://www.lix.polytechnique.fr/~nikolentzos/files/meladianos_ecir18\n",
    "\n",
    "        1) Removing retweets\n",
    "        2) Removing duplicates\n",
    "        3) Removing @ mentions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting tweet preprocessing...\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Initial data filtering\n",
    "    print(\"\\nFiltering tweets...\")\n",
    "    initial_count = len(processed_df)\n",
    "\n",
    "    # 1. Remove retweets\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.startswith(\"RT \", na=False)]\n",
    "    retweets_removed = initial_count - len(processed_df)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"Tweet\"])\n",
    "    duplicates_removed = initial_count - retweets_removed - len(processed_df)\n",
    "\n",
    "    # 3. Remove tweets with @-mentions\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.contains(\"@\", na=False)]\n",
    "    mentions_removed = (\n",
    "        initial_count - retweets_removed - duplicates_removed - len(processed_df)\n",
    "    )\n",
    "\n",
    "    # Print filtering statistics\n",
    "    print(f\"Removed {retweets_removed} retweets\")\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Removed {mentions_removed} tweets with @-mentions\")\n",
    "    print(f\"Remaining tweets: {len(processed_df)}\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(len(processed_df) / batch_size))\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    processed_tweets = []\n",
    "    with tqdm(total=len(processed_df), desc=\"Processing tweets\") as pbar:\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(processed_df))\n",
    "\n",
    "            # Get current batch\n",
    "            batch = processed_df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "            # Process batch\n",
    "            batch_results = [preprocess_text(tweet) for tweet in batch]\n",
    "            processed_tweets.extend(batch_results)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    # Add processed tweets to DataFrame\n",
    "    processed_df[\"Tweet\"] = processed_tweets\n",
    "\n",
    "    # Print timing statistics\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per tweet: {total_time/len(processed_df):.4f} seconds\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs standard text preprocessing tasks:\n",
    "    1. Tokenization\n",
    "    2. Stopword removal\n",
    "    3. Punctuation and special character removal\n",
    "    4. URL removal\n",
    "    5. Porter stemming\n",
    "\n",
    "    Args:\n",
    "        text: String containing the tweet text\n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Tokenization : better tokenization through word_tokenize by NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization is kept (porter stemming less precise)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 5056050\n",
      "Preprocessing tweets...\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 2619447 retweets\n",
      "Removed 120425 duplicates\n",
      "Removed 464706 tweets with @-mentions\n",
      "Remaining tweets: 1851472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets:  17%|█▋        | 318000/1851472 [01:41<08:15, 3093.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets:  96%|█████████▌| 1778000/1851472 [09:26<00:23, 3141.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing tweets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m tweet_processing_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_preprocess_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtweet_processing_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m300\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36mbatch_preprocess_tweets\u001b[0;34m(df, batch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m batch \u001b[38;5;241m=\u001b[39m processed_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[start_idx:end_idx]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Process batch\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     67\u001b[0m processed_tweets\u001b[38;5;241m.\u001b[39mextend(batch_results)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m batch \u001b[38;5;241m=\u001b[39m processed_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[start_idx:end_idx]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Process batch\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     67\u001b[0m processed_tweets\u001b[38;5;241m.\u001b[39mextend(batch_results)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 115\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    112\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    116\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Lemmatization is kept (porter stemming less precise)\u001b[39;00m\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     23\u001b[0m     ]\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/nltk/corpus/reader/api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/nltk/corpus/reader/api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[0;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/nltk/data.py:323\u001b[0m, in \u001b[0;36mFileSystemPathPointer.open\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 323\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m         stream \u001b[38;5;241m=\u001b[39m SeekableUnicodeStreamReader(stream, encoding)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"challenge_data/train_tweets\"):\n",
    "    df = pd.read_csv(\"challenge_data/train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "print(f\"Preprocessing tweets...\")\n",
    "tweet_processing_start = time.time()\n",
    "df = batch_preprocess_tweets(df)\n",
    "print(f\"Preprocessing took {time.time() - tweet_processing_start:.2f} seconds\")\n",
    "print(df.head(300))\n",
    "df.to_csv(\"preprocessed_tweets_.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>PeriodID</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>hope argentina lose would fun see belgium go f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>watch argentina v belgium th july live go link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>even though hate belgium beating u waffle damn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>lionel messi scored assisted goal world cup ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1404575400000</td>\n",
       "      <td>three new player argentina teamone calamitious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851467</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>sound like quite match morning bad know work f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851468</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>ok england tie score u advance right worldcup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851469</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>real soccer match many boring game worldcup us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851470</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>woah awesome game soccer played worldcup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851471</th>\n",
       "      <td>18_129</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>1276876799000</td>\n",
       "      <td>ugh shouldve usa worldcup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1851471 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "0          11_0       11         0          0  1404575400000   \n",
       "1          11_0       11         0          0  1404575400000   \n",
       "2          11_0       11         0          0  1404575400000   \n",
       "3          11_0       11         0          0  1404575400000   \n",
       "4          11_0       11         0          0  1404575400000   \n",
       "...         ...      ...       ...        ...            ...   \n",
       "1851467  18_129       18       129          0  1276876799000   \n",
       "1851468  18_129       18       129          0  1276876799000   \n",
       "1851469  18_129       18       129          0  1276876799000   \n",
       "1851470  18_129       18       129          0  1276876799000   \n",
       "1851471  18_129       18       129          0  1276876799000   \n",
       "\n",
       "                                                     Tweet  \n",
       "0        hope argentina lose would fun see belgium go f...  \n",
       "1        watch argentina v belgium th july live go link...  \n",
       "2        even though hate belgium beating u waffle damn...  \n",
       "3        lionel messi scored assisted goal world cup ar...  \n",
       "4        three new player argentina teamone calamitious...  \n",
       "...                                                    ...  \n",
       "1851467  sound like quite match morning bad know work f...  \n",
       "1851468      ok england tie score u advance right worldcup  \n",
       "1851469  real soccer match many boring game worldcup us...  \n",
       "1851470           woah awesome game soccer played worldcup  \n",
       "1851471                          ugh shouldve usa worldcup  \n",
       "\n",
       "[1851471 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"preprocessed_tweets.csv\")\n",
    "original_count = len(df)\n",
    "df = df.dropna() \n",
    "rows_dropped = original_count - len(df)\n",
    "print(f\"Rows dropped {rows_dropped}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tweet embeddings...\n",
      "Embedding computation took 34.80 seconds\n"
     ]
    }
   ],
   "source": [
    "vector_size = 50  # Adjust based on the chosen GloVe model\n",
    "print(f\"Computing tweet embeddings...\")\n",
    "embedding_start = time.time()\n",
    "tweet_vectors = np.vstack(\n",
    "    [get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df[\"Tweet\"]]\n",
    ")\n",
    "print(f\"Embedding computation took {time.time() - embedding_start:.2f} seconds\")\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = (\n",
    "    period_features.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_features.isna().sum().sum()\n",
    "period_features.to_csv(\"period_features_glove_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_features = pd.read_csv(\"period_features_glove_50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MatchID  PeriodID   ID         0         1         2         3         4  \\\n",
      "0        6         0  6_0  0.163735  0.220645  0.072085 -0.371477 -0.026980   \n",
      "1        6         1  6_1  0.180883  0.237059  0.078532 -0.384263 -0.038826   \n",
      "2        6         2  6_2  0.201530  0.170647  0.078877 -0.399935  0.052759   \n",
      "3        6         3  6_3  0.173831  0.247219  0.072275 -0.401311 -0.054211   \n",
      "4        6         4  6_4  0.184585  0.270589  0.077353 -0.409590 -0.059731   \n",
      "\n",
      "          5         6  ...        40        41        42        43        44  \\\n",
      "0  0.119863  0.323808  ... -0.804271 -0.164048  0.187336  0.186597  0.122361   \n",
      "1  0.106418  0.310424  ... -0.816587 -0.171208  0.185524  0.204467  0.108616   \n",
      "2  0.142922  0.293746  ... -0.761844 -0.150491  0.148775  0.255487  0.171711   \n",
      "3  0.087987  0.309758  ... -0.796276 -0.160897  0.178360  0.221406  0.104271   \n",
      "4  0.094002  0.300453  ... -0.814469 -0.145223  0.178493  0.240605  0.096409   \n",
      "\n",
      "         45        46        47        48        49  \n",
      "0 -0.145035 -0.068987 -0.224695  0.130825 -0.100818  \n",
      "1 -0.137737 -0.068262 -0.226837  0.139591 -0.109903  \n",
      "2 -0.132164 -0.078140 -0.186125  0.210200 -0.137517  \n",
      "3 -0.145334 -0.071579 -0.238256  0.152471 -0.093769  \n",
      "4 -0.124179 -0.070581 -0.231502  0.161077 -0.088499  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "vector_size = 50  # Adjust based on the chosen GloVe model\n",
    "# Concatenate all evaluation data first\n",
    "eval_dfs = []\n",
    "for fname in os.listdir(\"challenge_data/eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"challenge_data/eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "    eval_dfs.append(val_df)\n",
    "\n",
    "eval_df = pd.concat(eval_dfs, ignore_index=True)\n",
    "\n",
    "# Compute embeddings for the concatenated evaluation data\n",
    "tweet_vectors_test = np.vstack(\n",
    "    [get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in eval_df[\"Tweet\"]]\n",
    ")\n",
    "tweet_df_test = pd.DataFrame(tweet_vectors_test)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features_test = pd.concat([eval_df, tweet_df_test], axis=1)\n",
    "period_features_test = period_features_test.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "\n",
    "# Group the tweets into their corresponding periods\n",
    "period_features_test = (\n",
    "    period_features_test.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")\n",
    "print(period_features_test.head())\n",
    "period_features_test.to_csv(\"period_features_test_glove_50.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
