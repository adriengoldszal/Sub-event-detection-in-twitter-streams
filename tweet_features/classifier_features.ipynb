{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses preprocessed training data to train different models in order\n",
    "### to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_features = pd.read_csv('match_analysis_preprocessed_tweets.csv') # preprocessing using Glove and dropping certain types of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>EventType</th>\n",
       "      <th>MinuteRenormalized</th>\n",
       "      <th>TweetCountPercentage</th>\n",
       "      <th>EventProportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117460</td>\n",
       "      <td>0.063492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.117460</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.203175</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.330159</td>\n",
       "      <td>0.023077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>19_125.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968992</td>\n",
       "      <td>0.451751</td>\n",
       "      <td>0.019643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>19_126.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.407344</td>\n",
       "      <td>0.027559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>19_127.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984496</td>\n",
       "      <td>0.370623</td>\n",
       "      <td>0.040860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>19_128.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>0.320239</td>\n",
       "      <td>0.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>19_129.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.263877</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2149 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  MatchID  EventType  MinuteRenormalized  TweetCountPercentage  \\\n",
       "0        0_0.0        0          0            0.000000              0.117460   \n",
       "1        0_1.0        0          0            0.007752              0.117460   \n",
       "2        0_2.0        0          0            0.015504              0.155556   \n",
       "3        0_3.0        0          0            0.023256              0.203175   \n",
       "4        0_4.0        0          0            0.031008              0.330159   \n",
       "...        ...      ...        ...                 ...                   ...   \n",
       "2144  19_125.0       19          1            0.968992              0.451751   \n",
       "2145  19_126.0       19          1            0.976744              0.407344   \n",
       "2146  19_127.0       19          1            0.984496              0.370623   \n",
       "2147  19_128.0       19          1            0.992248              0.320239   \n",
       "2148  19_129.0       19          1            1.000000              0.263877   \n",
       "\n",
       "      EventProportion  \n",
       "0            0.063492  \n",
       "1            0.047619  \n",
       "2            0.053333  \n",
       "3            0.033333  \n",
       "4            0.023077  \n",
       "...               ...  \n",
       "2144         0.019643  \n",
       "2145         0.027559  \n",
       "2146         0.040860  \n",
       "2147         0.051724  \n",
       "2148         0.050000  \n",
       "\n",
       "[2149 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2149, 3)\n"
     ]
    }
   ],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=[\"MatchID\",\"ID\", \"EventType\"]).values\n",
    "print(X.shape)\n",
    "# We extract the labels of our training samples\n",
    "y = period_features[\"EventType\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Evaluating on a test set:\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_features_test = pd.read_csv(\"match_analysis_eval_cleaned_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MatchID</th>\n",
       "      <th>MinuteRenormalized</th>\n",
       "      <th>TweetCountPercentage</th>\n",
       "      <th>EventProportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6_0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143348</td>\n",
       "      <td>0.037356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6_1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.152790</td>\n",
       "      <td>0.051351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.150215</td>\n",
       "      <td>0.041209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6_3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.042827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6_4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.261803</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>16_125.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.968992</td>\n",
       "      <td>0.328482</td>\n",
       "      <td>0.057357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>16_126.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.057743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>16_127.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.984496</td>\n",
       "      <td>0.287942</td>\n",
       "      <td>0.038674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>16_128.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.992248</td>\n",
       "      <td>0.252599</td>\n",
       "      <td>0.030488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>16_129.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212058</td>\n",
       "      <td>0.044983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  MatchID  MinuteRenormalized  TweetCountPercentage  \\\n",
       "0       6_0.0        6            0.000000              0.143348   \n",
       "1       6_1.0        6            0.007752              0.152790   \n",
       "2       6_2.0        6            0.015504              0.150215   \n",
       "3       6_3.0        6            0.023256              0.194421   \n",
       "4       6_4.0        6            0.031008              0.261803   \n",
       "..        ...      ...                 ...                   ...   \n",
       "513  16_125.0       16            0.968992              0.328482   \n",
       "514  16_126.0       16            0.976744              0.307692   \n",
       "515  16_127.0       16            0.984496              0.287942   \n",
       "516  16_128.0       16            0.992248              0.252599   \n",
       "517  16_129.0       16            1.000000              0.212058   \n",
       "\n",
       "     EventProportion  \n",
       "0           0.037356  \n",
       "1           0.051351  \n",
       "2           0.041209  \n",
       "3           0.042827  \n",
       "4           0.051282  \n",
       "..               ...  \n",
       "513         0.057357  \n",
       "514         0.057743  \n",
       "515         0.038674  \n",
       "516         0.030488  \n",
       "517         0.044983  \n",
       "\n",
       "[518 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:  0.6232558139534884\n"
     ]
    }
   ],
   "source": [
    "# We set up a basic classifier that we train and then calculate the accuracy on our test set\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n",
    "# We add a dummy classifier for sanity purposes\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "X_eval = period_features_test.drop(columns=[\"ID\", \"MatchID\"]).values\n",
    "\n",
    "\n",
    "# Predict using the trained classifiers\n",
    "preds = clf.predict(X_eval)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "period_features_test[\"EventType\"] = preds\n",
    "\n",
    "# Prepare the final prediction dataframes\n",
    "predictions = period_features_test[[\"ID\", \"EventType\"]]\n",
    "\n",
    "pred_df = predictions\n",
    "pred_df.to_csv(\"logistic_better_preprocessing_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:  0.6976744186046512\n"
     ]
    }
   ],
   "source": [
    "period_features_test = pd.read_csv(\"match_analysis_eval_cleaned_tweets.csv\")\n",
    "# We set up a basic classifier that we train and then calculate the accuracy on our test set\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# This time we train our classifier on the full dataset that it is available to us.\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100).fit(X, y)\n",
    "\n",
    "X_eval = period_features_test.drop(columns=[\"MatchID\", \"ID\"]).values\n",
    "\n",
    "\n",
    "# Predict using the trained classifiers\n",
    "preds = clf.predict(X_eval)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "period_features_test[\"EventType\"] = preds\n",
    "\n",
    "# Prepare the final prediction dataframes\n",
    "predictions = period_features_test[[\"ID\", \"EventType\"]]\n",
    "\n",
    "pred_df = predictions\n",
    "pred_df.to_csv(\"rf_better_preprocessing_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (SVM): 0.648062015503876\n",
      "Cross-validation scores: [0.62790698 0.6179402  0.6744186  0.6744186  0.59666667]\n",
      "Mean CV score: 0.6382702104097453\n",
      "Confusion Matrix:\n",
      "[[183 125]\n",
      " [102 235]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.59      0.62       308\n",
      "           1       0.65      0.70      0.67       337\n",
      "\n",
      "    accuracy                           0.65       645\n",
      "   macro avg       0.65      0.65      0.65       645\n",
      "weighted avg       0.65      0.65      0.65       645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "period_features_test = pd.read_csv(\"match_analysis_eval_cleaned_tweets.csv\")\n",
    "# Train the SVM classifier on the train set\n",
    "svm_clf = SVC(random_state=42, kernel='rbf', probability=True).fit(X_train, y_train)\n",
    "\n",
    "# Test set prediction and evaluation\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(\"Test set accuracy (SVM):\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm_clf, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean CV score:\", scores.mean())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Train the SVM classifier on the full dataset\n",
    "svm_clf = SVC(random_state=42, kernel='rbf', probability=True).fit(X, y)\n",
    "\n",
    "# Add a dummy classifier for sanity purposes\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "# Prepare the evaluation dataset\n",
    "X_eval = period_features_test.drop(columns=[\"MatchID\",\"ID\"]).values\n",
    "\n",
    "# Predict using the trained SVM classifier\n",
    "svm_preds = svm_clf.predict(X_eval)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "period_features_test[\"EventType\"] = svm_preds\n",
    "\n",
    "# Prepare the final prediction dataframe\n",
    "predictions = period_features_test[[\"ID\", \"EventType\"]]\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "pred_df = predictions\n",
    "pred_df.to_csv(\"svm_bert_predictions_glove.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (Bagged SVM): 0.6341085271317829\n",
      "Cross-validation scores: [0.63787375 0.6179402  0.67109635 0.6744186  0.59666667]\n",
      "Mean CV score: 0.6395991140642303\n",
      "Confusion Matrix:\n",
      "[[165 143]\n",
      " [ 93 244]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.54      0.58       308\n",
      "           1       0.63      0.72      0.67       337\n",
      "\n",
      "    accuracy                           0.63       645\n",
      "   macro avg       0.64      0.63      0.63       645\n",
      "weighted avg       0.63      0.63      0.63       645\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['PeriodID'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m dummy_clf \u001b[38;5;241m=\u001b[39m DummyClassifier(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Prepare the evaluation dataset\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m X_eval \u001b[38;5;241m=\u001b[39m \u001b[43mperiod_features_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMatchID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPeriodID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Predict using the trained bagged SVM classifier\u001b[39;00m\n\u001b[1;32m     50\u001b[0m svm_preds \u001b[38;5;241m=\u001b[39m bagged_svm\u001b[38;5;241m.\u001b[39mpredict(X_eval)\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['PeriodID'] not found in axis\""
     ]
    }
   ],
   "source": [
    "period_features_test = pd.read_csv(\"match_analysis_eval_cleaned_tweets.csv\")\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create base SVM classifier\n",
    "base_svm = SVC(random_state=42, kernel='rbf', probability=True)\n",
    "\n",
    "# Create bagged SVM classifier\n",
    "bagged_svm = BaggingClassifier(\n",
    "    estimator=base_svm,\n",
    "    n_estimators=70,  # you can adjust this number\n",
    "    max_samples=0.8,  # you can adjust this fraction\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the bagged SVM classifier\n",
    "bagged_svm.fit(X_train, y_train)\n",
    "\n",
    "# Test set prediction and evaluation\n",
    "y_pred = bagged_svm.predict(X_test)\n",
    "print(\"Test set accuracy (Bagged SVM):\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(bagged_svm, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean CV score:\", scores.mean())\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "y_pred = bagged_svm.predict(X_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Train the bagged SVM classifier on the full dataset\n",
    "bagged_svm = BaggingClassifier(\n",
    "    estimator=base_svm,\n",
    "    n_estimators=70,\n",
    "    max_samples=0.8,\n",
    "    random_state=42\n",
    ").fit(X, y)\n",
    "\n",
    "# Add a dummy classifier for sanity purposes\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "# Prepare the evaluation dataset\n",
    "X_eval = period_features_test.drop(columns=[\"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "\n",
    "# Predict using the trained bagged SVM classifier\n",
    "svm_preds = bagged_svm.predict(X_eval)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "period_features_test[\"EventType\"] = svm_preds\n",
    "\n",
    "# Prepare the final prediction dataframe\n",
    "predictions = period_features_test[[\"ID\", \"EventType\"]]\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "pred_df = predictions\n",
    "pred_df.to_csv(\"bagged_svm_rbf_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy (XGBoost): 0.7757009345794392\n",
      "Cross-validation scores: [0.74916388 0.71571906 0.74916388 0.74247492 0.7458194 ]\n",
      "Mean CV score: 0.7404682274247492\n",
      "Confusion Matrix:\n",
      "[[226  74]\n",
      " [ 70 272]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.75      0.76       300\n",
      "         1.0       0.79      0.80      0.79       342\n",
      "\n",
      "    accuracy                           0.78       642\n",
      "   macro avg       0.77      0.77      0.77       642\n",
      "weighted avg       0.78      0.78      0.78       642\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 768, got 200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m X_eval \u001b[38;5;241m=\u001b[39m period_features_test\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatchID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeriodID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Predict using the trained XGBoost classifier\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m xgb_preds \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Add predictions to the dataframe\u001b[39;00m\n\u001b[1;32m     44\u001b[0m period_features_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEventType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m xgb_preds\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/xgboost/sklearn.py:1565\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     X: ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     iteration_range: Optional[IterationRange] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1563\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[0;32m-> 1565\u001b[0m         class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[1;32m   1573\u001b[0m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/xgboost/sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[1;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m/Data/adrien.goldszal/data_challenge/lib/python3.11/site-packages/xgboost/core.py:2520\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2517\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2518\u001b[0m         )\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2521\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2522\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2523\u001b[0m         )\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 768, got 200"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Load test dataset\n",
    "period_features_test = pd.read_csv(\"period_features_test_glove.csv\")\n",
    "\n",
    "# Train the XGBoost classifier on the train set\n",
    "xgb_clf = XGBClassifier(\n",
    "    random_state=42,\n",
    "    learning_rate=0.05,  # Reduced\n",
    "    n_estimators=200,    # Increased\n",
    "    max_depth=3,         # Reduced to prevent overfitting\n",
    "    min_child_weight=3,  # Helps with overfitting\n",
    "    subsample=0.8,       # Use 80% of data per tree\n",
    "    colsample_bytree=0.8 # Use 80% of features per tree\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# Test set prediction and evaluation\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "print(\"Test set accuracy (XGBoost):\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(xgb_clf, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean CV score:\", scores.mean())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Prepare the evaluation dataset\n",
    "X_eval = period_features_test.drop(columns=[\"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "\n",
    "# Predict using the trained XGBoost classifier\n",
    "xgb_preds = xgb_clf.predict(X_eval).astype(float)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "period_features_test[\"EventType\"] = xgb_preds\n",
    "\n",
    "# Prepare the final prediction dataframe\n",
    "predictions = period_features_test[[\"ID\", \"EventType\"]]\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "pred_df = predictions\n",
    "pred_df.to_csv(\"xgboost_predictions.csv\", index=False)\n",
    "\n",
    "# Optional: Print feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': range(X.shape[1]),\n",
    "    'importance': xgb_clf.feature_importances_\n",
    "})\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
