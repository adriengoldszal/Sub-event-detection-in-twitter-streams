{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Data/adrien.goldszal/data_challenge/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT\n",
    "1) Simpler text processing because BERT has it's own tokenizer and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess_tweets(df, batch_size=1000):\n",
    "    \"\"\"Main preprocessing function with filtering and batching\n",
    "    Link here https://www.lix.polytechnique.fr/~nikolentzos/files/meladianos_ecir18\n",
    "\n",
    "        1) Removing retweets\n",
    "        2) Removing duplicates\n",
    "        3) Removing @ mentions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting tweet preprocessing...\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Initial data filtering\n",
    "    print(\"\\nFiltering tweets...\")\n",
    "    initial_count = len(processed_df)\n",
    "\n",
    "    # 1. Remove retweets\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.startswith(\"RT \", na=False)]\n",
    "    retweets_removed = initial_count - len(processed_df)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"Tweet\"])\n",
    "    duplicates_removed = initial_count - retweets_removed - len(processed_df)\n",
    "\n",
    "    # 3. Remove tweets with @-mentions\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.contains(\"@\", na=False)]\n",
    "    mentions_removed = (\n",
    "        initial_count - retweets_removed - duplicates_removed - len(processed_df)\n",
    "    )\n",
    "\n",
    "    # Print filtering statistics\n",
    "    print(f\"Removed {retweets_removed} retweets\")\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Removed {mentions_removed} tweets with @-mentions\")\n",
    "    print(f\"Remaining tweets: {len(processed_df)}\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(len(processed_df) / batch_size))\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    processed_tweets = []\n",
    "    with tqdm(total=len(processed_df), desc=\"Processing tweets\") as pbar:\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(processed_df))\n",
    "\n",
    "            # Get current batch\n",
    "            batch = processed_df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "            # Process batch\n",
    "            batch_results = [preprocess_text(tweet) for tweet in batch]\n",
    "            processed_tweets.extend(batch_results)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    # Add processed tweets to DataFrame\n",
    "    processed_df[\"Tweet\"] = processed_tweets\n",
    "\n",
    "    # Print timing statistics\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per tweet: {total_time/len(processed_df):.4f} seconds\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Limited preprocessing for BERT\n",
    "\n",
    "    Args:\n",
    "        text: String containing the tweet text\n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 5056050\n",
      "Preprocessing tweets...\n",
      "Starting tweet preprocessing...\n",
      "\n",
      "Filtering tweets...\n",
      "Removed 2619447 retweets\n",
      "Removed 120425 duplicates\n",
      "Removed 464706 tweets with @-mentions\n",
      "Remaining tweets: 1851472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 1851472/1851472 [00:03<00:00, 611542.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Total processing time: 5.77 seconds\n",
      "Average time per tweet: 0.0000 seconds\n",
      "Preprocessing took 5.83 seconds\n",
      "       ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "6    11_0       11         0          0  1404575400000   \n",
      "7    11_0       11         0          0  1404575400000   \n",
      "9    11_0       11         0          0  1404575400000   \n",
      "10   11_0       11         0          0  1404575400000   \n",
      "12   11_0       11         0          0  1404575400000   \n",
      "..    ...      ...       ...        ...            ...   \n",
      "789  11_0       11         0          0  1404575419000   \n",
      "792  11_0       11         0          0  1404575419000   \n",
      "794  11_0       11         0          0  1404575419000   \n",
      "795  11_0       11         0          0  1404575419000   \n",
      "797  11_0       11         0          0  1404575419000   \n",
      "\n",
      "                                                 Tweet  \n",
      "6    I just hope Argentina lose. Would be fun to se...  \n",
      "7    Watch Argentina vs Belgium 5th July 2014 LIVE ...  \n",
      "9    Even though I hate Belgium for beating the US,...  \n",
      "10   Lionel Messi has scored 4 and assisted 1 of th...  \n",
      "12   Three new players for the Argentina team,one o...  \n",
      "..                                                 ...  \n",
      "789     Argentina and Netherlands winning today #leggo  \n",
      "792  With #Belgium tonight! Want that young brigade...  \n",
      "794  All ready for the first kick of tonight's firs...  \n",
      "795  1-0 to #BEL??? Hazard the scorer??? Go for it,...  \n",
      "797  Argentina Belgium will go to penalties i recko...  \n",
      "\n",
      "[300 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"../challenge_data/train_tweets\"):\n",
    "    df = pd.read_csv(\"../challenge_data/train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "print(f\"Preprocessing tweets...\")\n",
    "tweet_processing_start = time.time()\n",
    "df = batch_preprocess_tweets(df)\n",
    "print(f\"Preprocessing took {time.time() - tweet_processing_start:.2f} seconds\")\n",
    "print(df.head(300))\n",
    "df.to_csv(\"bert_preprocessed_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv and check if there are NaN values just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"bert_preprocessed_tweets.csv\")\n",
    "original_count = len(df)\n",
    "df = df.dropna() \n",
    "rows_dropped = original_count - len(df)\n",
    "print(f\"Rows dropped {rows_dropped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT tokenizer and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function to get BERT embeddings for a batch of tweets\n",
    "def get_bert_embeddings(tweets, tokenizer, model, max_length=128):\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize all tweets in the batch\n",
    "    encoded = tokenizer(\n",
    "        tweets,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the same device as model\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # Use [CLS] token embedding (first token) as sentence representation\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# 3. Process your tweets in batches\n",
    "def process_tweets_with_bert(df, batch_size=1000):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch = df['Tweet'].iloc[i:i + batch_size].tolist()\n",
    "        batch_embeddings = get_bert_embeddings(batch, tokenizer, model)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "    embeddings_array = np.array(all_embeddings, dtype=np.float16)\n",
    "    \n",
    "    return embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1852/1852 [28:21<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (1851472, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = process_tweets_with_bert(df)\n",
    "np.savez_compressed('embeddings.npz', embeddings=embeddings)\n",
    "    \n",
    "# Now embeddings contains BERT representations for all your tweets\n",
    "# Shape will be (n_tweets, 768) for bert-base-uncased\n",
    "print(f\"Generated embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.070923</td>\n",
       "      <td>0.085999</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>-0.235107</td>\n",
       "      <td>-0.509766</td>\n",
       "      <td>-0.553223</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>0.690430</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.596191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.130981</td>\n",
       "      <td>0.434570</td>\n",
       "      <td>-0.333008</td>\n",
       "      <td>0.365234</td>\n",
       "      <td>0.096069</td>\n",
       "      <td>-0.083862</td>\n",
       "      <td>-0.495605</td>\n",
       "      <td>0.678711</td>\n",
       "      <td>0.731934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.031891</td>\n",
       "      <td>-0.261475</td>\n",
       "      <td>0.156982</td>\n",
       "      <td>-0.210327</td>\n",
       "      <td>-0.327881</td>\n",
       "      <td>-0.526855</td>\n",
       "      <td>0.488037</td>\n",
       "      <td>0.681641</td>\n",
       "      <td>-0.308838</td>\n",
       "      <td>-0.073120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040131</td>\n",
       "      <td>-0.271729</td>\n",
       "      <td>0.027756</td>\n",
       "      <td>-0.272217</td>\n",
       "      <td>0.067444</td>\n",
       "      <td>0.081604</td>\n",
       "      <td>-0.389160</td>\n",
       "      <td>-0.597656</td>\n",
       "      <td>0.498535</td>\n",
       "      <td>0.389893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003460</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>-0.145874</td>\n",
       "      <td>-0.166748</td>\n",
       "      <td>-0.283691</td>\n",
       "      <td>-0.163452</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.790527</td>\n",
       "      <td>0.143799</td>\n",
       "      <td>-0.282227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.552734</td>\n",
       "      <td>-0.106445</td>\n",
       "      <td>0.035675</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>-0.124084</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>0.605469</td>\n",
       "      <td>0.257812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.358154</td>\n",
       "      <td>-0.099182</td>\n",
       "      <td>0.340576</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>-0.898926</td>\n",
       "      <td>-0.537109</td>\n",
       "      <td>0.199585</td>\n",
       "      <td>0.351318</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.191406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573730</td>\n",
       "      <td>0.287842</td>\n",
       "      <td>0.367188</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>0.493896</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>-0.477051</td>\n",
       "      <td>-0.358398</td>\n",
       "      <td>0.668945</td>\n",
       "      <td>0.874023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.351562</td>\n",
       "      <td>0.234253</td>\n",
       "      <td>-0.267090</td>\n",
       "      <td>-0.241455</td>\n",
       "      <td>-0.561035</td>\n",
       "      <td>-0.253418</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>0.620605</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>-0.574707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>0.059631</td>\n",
       "      <td>0.315918</td>\n",
       "      <td>0.168457</td>\n",
       "      <td>-0.021912</td>\n",
       "      <td>0.214111</td>\n",
       "      <td>-0.436279</td>\n",
       "      <td>-0.312012</td>\n",
       "      <td>0.735352</td>\n",
       "      <td>0.274902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.334717</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.021744</td>\n",
       "      <td>-0.214355</td>\n",
       "      <td>-0.740723</td>\n",
       "      <td>-0.620605</td>\n",
       "      <td>0.373779</td>\n",
       "      <td>0.886230</td>\n",
       "      <td>-0.114014</td>\n",
       "      <td>-0.408203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.085754</td>\n",
       "      <td>0.100769</td>\n",
       "      <td>-0.240967</td>\n",
       "      <td>0.623047</td>\n",
       "      <td>-0.327637</td>\n",
       "      <td>-0.142456</td>\n",
       "      <td>-0.615723</td>\n",
       "      <td>0.567383</td>\n",
       "      <td>0.297852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.235229</td>\n",
       "      <td>-0.693848</td>\n",
       "      <td>0.489746</td>\n",
       "      <td>-0.093689</td>\n",
       "      <td>-0.721191</td>\n",
       "      <td>-0.917969</td>\n",
       "      <td>0.422607</td>\n",
       "      <td>0.592773</td>\n",
       "      <td>-0.021332</td>\n",
       "      <td>-0.706055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021744</td>\n",
       "      <td>0.151855</td>\n",
       "      <td>0.147339</td>\n",
       "      <td>-0.305664</td>\n",
       "      <td>0.219971</td>\n",
       "      <td>-0.167725</td>\n",
       "      <td>-0.345459</td>\n",
       "      <td>-0.473389</td>\n",
       "      <td>0.631836</td>\n",
       "      <td>0.691406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.901367</td>\n",
       "      <td>-0.036652</td>\n",
       "      <td>0.188721</td>\n",
       "      <td>0.037048</td>\n",
       "      <td>-0.659180</td>\n",
       "      <td>-0.245972</td>\n",
       "      <td>0.620605</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>-0.956543</td>\n",
       "      <td>-0.093933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>0.462402</td>\n",
       "      <td>0.388916</td>\n",
       "      <td>-0.541016</td>\n",
       "      <td>-0.010513</td>\n",
       "      <td>-0.411865</td>\n",
       "      <td>-0.225708</td>\n",
       "      <td>-0.522949</td>\n",
       "      <td>0.579590</td>\n",
       "      <td>0.291992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.599609</td>\n",
       "      <td>-0.134277</td>\n",
       "      <td>0.186646</td>\n",
       "      <td>-0.035126</td>\n",
       "      <td>-0.394043</td>\n",
       "      <td>-0.307373</td>\n",
       "      <td>0.404297</td>\n",
       "      <td>0.479492</td>\n",
       "      <td>-0.507812</td>\n",
       "      <td>-0.466553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203857</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.308105</td>\n",
       "      <td>-0.172241</td>\n",
       "      <td>0.390137</td>\n",
       "      <td>-0.001609</td>\n",
       "      <td>-0.352295</td>\n",
       "      <td>-0.129028</td>\n",
       "      <td>0.179321</td>\n",
       "      <td>0.478271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.222168</td>\n",
       "      <td>0.256104</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>-0.051971</td>\n",
       "      <td>-0.638184</td>\n",
       "      <td>-0.462646</td>\n",
       "      <td>0.960449</td>\n",
       "      <td>0.396484</td>\n",
       "      <td>-0.138184</td>\n",
       "      <td>-0.177856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025345</td>\n",
       "      <td>-0.154419</td>\n",
       "      <td>0.229858</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.239502</td>\n",
       "      <td>-0.028458</td>\n",
       "      <td>-0.125610</td>\n",
       "      <td>-0.523926</td>\n",
       "      <td>0.149902</td>\n",
       "      <td>0.291992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.070923  0.085999  0.172852 -0.235107 -0.509766 -0.553223  0.402344   \n",
       "1 -0.031891 -0.261475  0.156982 -0.210327 -0.327881 -0.526855  0.488037   \n",
       "2 -0.003460  0.262939 -0.145874 -0.166748 -0.283691 -0.163452  0.488281   \n",
       "3 -0.358154 -0.099182  0.340576  0.011528 -0.898926 -0.537109  0.199585   \n",
       "4 -0.351562  0.234253 -0.267090 -0.241455 -0.561035 -0.253418  0.099854   \n",
       "5 -0.334717 -0.096680  0.021744 -0.214355 -0.740723 -0.620605  0.373779   \n",
       "6 -0.235229 -0.693848  0.489746 -0.093689 -0.721191 -0.917969  0.422607   \n",
       "7 -0.901367 -0.036652  0.188721  0.037048 -0.659180 -0.245972  0.620605   \n",
       "8 -0.599609 -0.134277  0.186646 -0.035126 -0.394043 -0.307373  0.404297   \n",
       "9 -0.222168  0.256104  0.035034 -0.051971 -0.638184 -0.462646  0.960449   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.690430 -0.065430 -0.596191  ... -0.298096 -0.130981  0.434570 -0.333008   \n",
       "1  0.681641 -0.308838 -0.073120  ... -0.040131 -0.271729  0.027756 -0.272217   \n",
       "2  0.790527  0.143799 -0.282227  ... -0.552734 -0.106445  0.035675 -0.293945   \n",
       "3  0.351318 -0.593750 -0.191406  ...  0.573730  0.287842  0.367188 -0.207031   \n",
       "4  0.620605 -0.267578 -0.574707  ... -0.111816  0.059631  0.315918  0.168457   \n",
       "5  0.886230 -0.114014 -0.408203  ...  0.075195  0.085754  0.100769 -0.240967   \n",
       "6  0.592773 -0.021332 -0.706055  ... -0.021744  0.151855  0.147339 -0.305664   \n",
       "7  0.819824 -0.956543 -0.093933  ...  0.595703  0.462402  0.388916 -0.541016   \n",
       "8  0.479492 -0.507812 -0.466553  ...  0.203857  0.006367  0.308105 -0.172241   \n",
       "9  0.396484 -0.138184 -0.177856  ...  0.025345 -0.154419  0.229858  0.059570   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.365234  0.096069 -0.083862 -0.495605  0.678711  0.731934  \n",
       "1  0.067444  0.081604 -0.389160 -0.597656  0.498535  0.389893  \n",
       "2  0.568848 -0.124084  0.099731  0.015854  0.605469  0.257812  \n",
       "3  0.493896  0.020203 -0.477051 -0.358398  0.668945  0.874023  \n",
       "4 -0.021912  0.214111 -0.436279 -0.312012  0.735352  0.274902  \n",
       "5  0.623047 -0.327637 -0.142456 -0.615723  0.567383  0.297852  \n",
       "6  0.219971 -0.167725 -0.345459 -0.473389  0.631836  0.691406  \n",
       "7 -0.010513 -0.411865 -0.225708 -0.522949  0.579590  0.291992  \n",
       "8  0.390137 -0.001609 -0.352295 -0.129028  0.179321  0.478271  \n",
       "9  0.239502 -0.028458 -0.125610 -0.523926  0.149902  0.291992  \n",
       "\n",
       "[10 rows x 768 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = np.load('embeddings.npz')\n",
    "embeddings = loaded['embeddings']\n",
    "tweet_vectors = pd.DataFrame(embeddings)\n",
    "tweet_vectors.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_vectors], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = (\n",
    "    period_features.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=[\"EventType\", \"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "# We extract the labels of our training samples\n",
    "y = period_features[\"EventType\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Evaluating on a test set:\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:  0.7725856697819314\n"
     ]
    }
   ],
   "source": [
    "# We set up a basic classifier that we train and then calculate the accuracy on our test set\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../challenge_data/eval_tweets\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Read and preprocess test file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     val_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../challenge_data/eval_tweets/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname)\n\u001b[0;32m---> 12\u001b[0m     val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mpreprocess_text\u001b[49m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Get BERT embeddings for this file\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Note: using your existing functions\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m process_tweets_with_bert(val_df, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Train classifiers\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "\n",
    "# Process each file in test set\n",
    "for fname in os.listdir(\"../challenge_data/eval_tweets\"):\n",
    "    # Read and preprocess test file\n",
    "    val_df = pd.read_csv(\"../challenge_data/eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "    \n",
    "    # Get BERT embeddings for this file\n",
    "    # Note: using your existing functions\n",
    "    embeddings = process_tweets_with_bert(val_df, batch_size=1000)\n",
    "    \n",
    "    # Create DataFrame from embeddings\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embeddings,\n",
    "        columns=[f'embed_{i}' for i in range(embeddings.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Combine with original features\n",
    "    period_features_test = pd.concat([val_df, embedding_df], axis=1)\n",
    "    period_features_test = period_features_test.dropna()\n",
    "    \n",
    "    # Drop non-feature columns\n",
    "    period_features_test = period_features_test.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "    \n",
    "    # Group by match and period\n",
    "    period_features_test = (\n",
    "        period_features_test.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    "    )\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    X_test = period_features_test.drop(columns=[\"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    print(X_test.shape)\n",
    "    print(X_test.isna().sum())\n",
    "    X_test = X_test.dropna()\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = clf.predict(X_test)\n",
    "    dummy_preds = dummy_clf.predict(X_test)\n",
    "    \n",
    "    # Add predictions to DataFrame\n",
    "    period_features_test[\"EventType\"] = preds\n",
    "    period_features_test[\"DummyEventType\"] = dummy_preds\n",
    "    \n",
    "    # Append results\n",
    "    predictions.append(period_features_test[[\"ID\", \"EventType\"]])\n",
    "    dummy_predictions.append(period_features_test[[\"ID\", \"DummyEventType\"]])\n",
    "\n",
    "# Save predictions\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv(\"bert_logistic_predictions.csv\", index=False)\n",
    "\n",
    "pred_df = pd.concat(dummy_predictions)\n",
    "pred_df.to_csv(\"bert_dummy_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
