{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT\n",
    "1) Simpler text processing because BERT has it's own tokenizer and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess_tweets(df, batch_size=1000):\n",
    "    \"\"\"Main preprocessing function with filtering and batching\n",
    "    Link here https://www.lix.polytechnique.fr/~nikolentzos/files/meladianos_ecir18\n",
    "\n",
    "        1) Removing retweets\n",
    "        2) Removing duplicates\n",
    "        3) Removing @ mentions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting tweet preprocessing...\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Initial data filtering\n",
    "    print(\"\\nFiltering tweets...\")\n",
    "    initial_count = len(processed_df)\n",
    "\n",
    "    # 1. Remove retweets\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.startswith(\"RT \", na=False)]\n",
    "    retweets_removed = initial_count - len(processed_df)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"Tweet\"])\n",
    "    duplicates_removed = initial_count - retweets_removed - len(processed_df)\n",
    "\n",
    "    # 3. Remove tweets with @-mentions\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.contains(\"@\", na=False)]\n",
    "    mentions_removed = (\n",
    "        initial_count - retweets_removed - duplicates_removed - len(processed_df)\n",
    "    )\n",
    "\n",
    "    # Print filtering statistics\n",
    "    print(f\"Removed {retweets_removed} retweets\")\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Removed {mentions_removed} tweets with @-mentions\")\n",
    "    print(f\"Remaining tweets: {len(processed_df)}\")\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(len(processed_df) / batch_size))\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    processed_tweets = []\n",
    "    with tqdm(total=len(processed_df), desc=\"Processing tweets\") as pbar:\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(processed_df))\n",
    "\n",
    "            # Get current batch\n",
    "            batch = processed_df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "            # Process batch\n",
    "            batch_results = [preprocess_text(tweet) for tweet in batch]\n",
    "            processed_tweets.extend(batch_results)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    # Add processed tweets to DataFrame\n",
    "    processed_df[\"Tweet\"] = processed_tweets\n",
    "\n",
    "    # Print timing statistics\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per tweet: {total_time/len(processed_df):.4f} seconds\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Limited preprocessing for BERT\n",
    "\n",
    "    Args:\n",
    "        text: String containing the tweet text\n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all training files and concatenate them into one dataframe\n",
    "li = []\n",
    "for filename in os.listdir(\"../challenge_data/train_tweets\"):\n",
    "    df = pd.read_csv(\"../challenge_data/train_tweets/\" + filename)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, ignore_index=True)\n",
    "\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "print(f\"Preprocessing tweets...\")\n",
    "tweet_processing_start = time.time()\n",
    "df = batch_preprocess_tweets(df)\n",
    "print(f\"Preprocessing took {time.time() - tweet_processing_start:.2f} seconds\")\n",
    "print(df.head(300))\n",
    "df.to_csv(\"bert_preprocessed_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv and check if there are NaN values just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bert_preprocessed_tweets.csv\")\n",
    "original_count = len(df)\n",
    "df = df.dropna() \n",
    "rows_dropped = original_count - len(df)\n",
    "print(f\"Rows dropped {rows_dropped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT tokenizer and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function to get BERT embeddings for a batch of tweets\n",
    "def get_bert_embeddings(tweets, tokenizer, model, max_length=128):\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize all tweets in the batch\n",
    "    encoded = tokenizer(\n",
    "        tweets,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the same device as model\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # Use [CLS] token embedding (first token) as sentence representation\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# 3. Process your tweets in batches\n",
    "def process_tweets_with_bert(df, batch_size=1000):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch = df['Tweet'].iloc[i:i + batch_size].tolist()\n",
    "        batch_embeddings = get_bert_embeddings(batch, tokenizer, model)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "    embeddings_array = np.array(all_embeddings, dtype=np.float16)\n",
    "    \n",
    "    return embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = process_tweets_with_bert(df)\n",
    "np.savez_compressed('embeddings.npz', embeddings=embeddings)\n",
    "    \n",
    "# Now embeddings contains BERT representations for all your tweets\n",
    "# Shape will be (n_tweets, 768) for bert-base-uncased\n",
    "print(f\"Generated embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('embeddings.npz')\n",
    "embeddings = loaded['embeddings']\n",
    "tweet_vectors = pd.DataFrame(embeddings)\n",
    "tweet_vectors.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_vectors], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = (\n",
    "    period_features.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    ")\n",
    "period_features.to_csv(\"period_features_bert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=[\"EventType\", \"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "# We extract the labels of our training samples\n",
    "y = period_features[\"EventType\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Evaluating on a test set:\n",
    "\n",
    "# We split our data into a training and test set that we can use to train our classifier without fine-tuning into the\n",
    "# validation set and without submitting too many times into Kaggle\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up a basic classifier that we train and then calculate the accuracy on our test set\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test set: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "\n",
    "# Process each file in test set\n",
    "for fname in os.listdir(\"../challenge_data/eval_tweets\"):\n",
    "    # Read and preprocess test file\n",
    "    val_df = pd.read_csv(\"../challenge_data/eval_tweets/\" + fname)\n",
    "    val_df['Tweet'] = val_df['Tweet'].apply(preprocess_text)\n",
    "    \n",
    "    # Get BERT embeddings for this file\n",
    "    # Note: using your existing functions\n",
    "    embeddings = process_tweets_with_bert(val_df, batch_size=1000)\n",
    "    \n",
    "    # Create DataFrame from embeddings\n",
    "    embedding_df = pd.DataFrame(\n",
    "        embeddings,\n",
    "        columns=[f'embed_{i}' for i in range(embeddings.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Combine with original features\n",
    "    period_features_test = pd.concat([val_df, embedding_df], axis=1)\n",
    "    period_features_test = period_features_test.dropna()\n",
    "    \n",
    "    # Drop non-feature columns\n",
    "    period_features_test = period_features_test.drop(columns=[\"Timestamp\", \"Tweet\"])\n",
    "    \n",
    "    # Group by match and period\n",
    "    period_features_test = (\n",
    "        period_features_test.groupby([\"MatchID\", \"PeriodID\", \"ID\"]).mean().reset_index()\n",
    "    )\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    X_test = period_features_test.drop(columns=[\"MatchID\", \"PeriodID\", \"ID\"]).values\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    print(X_test.shape)\n",
    "    print(X_test.isna().sum())\n",
    "    X_test = X_test.dropna()\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = clf.predict(X_test)\n",
    "    dummy_preds = dummy_clf.predict(X_test)\n",
    "    \n",
    "    # Add predictions to DataFrame\n",
    "    period_features_test[\"EventType\"] = preds\n",
    "    period_features_test[\"DummyEventType\"] = dummy_preds\n",
    "    \n",
    "    # Append results\n",
    "    predictions.append(period_features_test[[\"ID\", \"EventType\"]])\n",
    "    dummy_predictions.append(period_features_test[[\"ID\", \"DummyEventType\"]])\n",
    "\n",
    "# Save predictions\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv(\"bert_logistic_predictions.csv\", index=False)\n",
    "\n",
    "pred_df = pd.concat(dummy_predictions)\n",
    "pred_df.to_csv(\"bert_dummy_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
