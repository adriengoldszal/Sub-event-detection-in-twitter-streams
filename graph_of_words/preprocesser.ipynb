{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import bisect\n",
    "from cvxpy import Variable, Minimize, norm, Problem\n",
    "from scipy.sparse import find\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /users/eleves-a/2022/adrien.goldszal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download some NLP models for processing, optional\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess_tweets(df, batch_size=1000):\n",
    "    \"\"\"Main preprocessing function with filtering and batching\n",
    "    Link here https://www.lix.polytechnique.fr/~nikolentzos/files/meladianos_ecir18\n",
    "\n",
    "        1) Removing retweets\n",
    "        2) Removing duplicates\n",
    "        3) Removing @ mentions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting tweet preprocessing...\")\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Initial data filtering\n",
    "    print(\"\\nFiltering tweets...\")\n",
    "    initial_count = len(processed_df)\n",
    "\n",
    "    # 1. Remove retweets\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.startswith(\"RT \", na=False)]\n",
    "    retweets_removed = initial_count - len(processed_df)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"Tweet\"])\n",
    "    duplicates_removed = initial_count - retweets_removed - len(processed_df)\n",
    "\n",
    "    # 3. Remove tweets with @-mentions\n",
    "    processed_df = processed_df[~processed_df[\"Tweet\"].str.contains(\"@\", na=False)]\n",
    "    mentions_removed = (\n",
    "        initial_count - retweets_removed - duplicates_removed - len(processed_df)\n",
    "    )\n",
    "\n",
    "    # Print filtering statistics\n",
    "    print(f\"Removed {retweets_removed} retweets\")\n",
    "    print(f\"Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"Removed {mentions_removed} tweets with @-mentions\")\n",
    "    print(f\"Remaining tweets: {len(processed_df)}\")\n",
    "\n",
    "    \n",
    "    vocabulary = set()\n",
    "    # Calculate number of batches\n",
    "    n_batches = int(np.ceil(len(processed_df) / batch_size))\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    processed_tweets = []\n",
    "    with tqdm(total=len(processed_df), desc=\"Processing tweets\") as pbar:\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(processed_df))\n",
    "\n",
    "            # Get current batch\n",
    "            batch = processed_df[\"Tweet\"].iloc[start_idx:end_idx]\n",
    "\n",
    "            # Process batch\n",
    "            batch_results, batch_vocab = zip(*[preprocess_text(tweet) for tweet in batch])\n",
    "            \n",
    "            for words in batch_vocab:\n",
    "                vocabulary.update(words)\n",
    "                \n",
    "            processed_tweets.extend(batch_results)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    \n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    \n",
    "    # Add processed tweets to DataFrame\n",
    "    processed_df[\"Tweet\"] = processed_tweets\n",
    "\n",
    "    # Print timing statistics\n",
    "    total_time = time.time() - total_start\n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per tweet: {total_time/len(processed_df):.4f} seconds\")\n",
    "\n",
    "    return processed_df, vocabulary\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs standard text preprocessing tasks:\n",
    "    1. Tokenization\n",
    "    2. Stopword removal\n",
    "    3. Punctuation and special character removal\n",
    "    4. URL removal\n",
    "    5. Porter stemming\n",
    "\n",
    "    Args:\n",
    "        text: String containing the tweet text\n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Simple splitting (as done in paper)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Porter stemming (paper uses this instead of lemmatization)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    return \" \".join(tokens), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(filename,save_dir=\"preprocessed_data\") :\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    base_name = os.path.basename(filename).replace('.csv', '')\n",
    "    save_path = os.path.join(save_dir, f\"{base_name}_preprocessed.pkl\")\n",
    "    \n",
    "    # Read match file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    processed_df, vocabulary = batch_preprocess_tweets(df)\n",
    "    \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'processed_df': processed_df,\n",
    "            'vocabulary': vocabulary\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Saved preprocessed data to {save_path}\")\n",
    "    \n",
    "    return processed_df, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacency_matrix_dense(tweets, vocabulary):\n",
    "    print(f'Generating adjacency matrix')\n",
    "    \"\"\"Method that is used to generate the adjacency matrix of the given tweets\"\"\"\n",
    "    wordsNumber = len(vocabulary)\n",
    "    adjacency_matrix = np.zeros((wordsNumber, wordsNumber))\n",
    "    tweets_edges = []\n",
    "    \n",
    "    for _, tweet in tweets['Tweet'].items():\n",
    "        # Convert tweet tokens to set to remove duplicates\n",
    "        tweet_words = set(tweet.split())  # Assuming tweet is space-separated string\n",
    "        \n",
    "        # Get indices of words in vocabulary\n",
    "        indexes = [bisect.bisect_left(vocabulary, word) for word in tweet_words]\n",
    "        \n",
    "        # Initialize edges list for this tweet\n",
    "        tweet_edges = []\n",
    "        \n",
    "        # Create edges between all word pairs\n",
    "        for i, idx1 in enumerate(indexes):\n",
    "            for j, idx2 in enumerate(indexes[i+1:], i+1):  # Start from i+1 to avoid duplicates\n",
    "                if idx1 != idx2:\n",
    "                    # Add weight to adjacency matrix\n",
    "                    weight = 1.0 / len(tweet_words)\n",
    "                    adjacency_matrix[idx1, idx2] += weight\n",
    "                    adjacency_matrix[idx2, idx1] += weight\n",
    "                    \n",
    "                    # Add edge to tweet edges\n",
    "                    tweet_edges.append(sorted([vocabulary[idx1], vocabulary[idx2]]))\n",
    "        \n",
    "        tweets_edges.append(tweet_edges)\n",
    "    \n",
    "    return adjacency_matrix, tweets_edges\n",
    "\n",
    "def get_edges_weight(adjacency_matrix, vocabulary, edges_list, nodes_list):\n",
    "    \"\"\"Method that is used to extract the weight for each edge in the given list. The nodes_list parameter is a\n",
    "    list that contains the nodes that are included in the given edges \"\"\"\n",
    "    \n",
    "    print(f'Getting edge weights ')\n",
    "    nodes = {}\n",
    "    for node in nodes_list:\n",
    "        index = bisect.bisect(vocabulary, node) - 1\n",
    "        if (0 <= index <= len(vocabulary)) and vocabulary[index] == node:\n",
    "            nodes[node] = index\n",
    "\n",
    "    weight_list = []\n",
    "    for edge in edges_list:\n",
    "        first_word, second_word = edge[0], edge[1]\n",
    "        if all(word in nodes for word in (first_word, second_word)):\n",
    "            indexes = [nodes[first_word], nodes[second_word]]\n",
    "            indexes.sort()\n",
    "            weight_list.append(adjacency_matrix[indexes[0], indexes[1]])\n",
    "        else:\n",
    "            weight_list.append(0)\n",
    "    return weight_list\n",
    "\n",
    "\n",
    "def get_nonzero_edges(matrix):\n",
    "    \"\"\"Method that is used to extract from the adjacency matrix the edges with no-negative weights\"\"\"\n",
    "    print(f'Getting non zero edges')\n",
    "    rows, columns, values = find(matrix)\n",
    "    return [[rows[i], columns[i], float(values[i])] for i in range(len(rows))]\n",
    "\n",
    "def generate_vector(adjacency_matrix, vocabulary):\n",
    "    \"\"\"Method that is used to generate a vector for the current period\"\"\"\n",
    "    print(f'Generating vector')\n",
    "    \n",
    "    non_zero_edges = get_nonzero_edges(adjacency_matrix)\n",
    "    vector = np.zeros((len(non_zero_edges), 1))\n",
    "    vector_edges = []\n",
    "    vector_nodes = set()\n",
    "    weighted_edges = {}\n",
    "    counter = 0\n",
    "    for row, column, value in non_zero_edges:\n",
    "        vector[counter] = value\n",
    "        nodes = [vocabulary[row], vocabulary[column]]\n",
    "        vector_edges.append(nodes)\n",
    "        vector_nodes.update(nodes)\n",
    "        weighted_edges[tuple(sorted(nodes))] = value\n",
    "        counter += 1\n",
    "    return vector, vector_nodes, vector_edges, weighted_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_event(current_period_data, previous_periods_data, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect if current period contains an event using Least Squares Optimization.\n",
    "    \n",
    "    Args:\n",
    "        current_period_data: Dict containing current period's data \n",
    "            (adjacency_matrix, vector, vector_nodes, vector_edges, tweets_edges, etc.)\n",
    "        previous_periods_data: List of dicts containing previous periods' data\n",
    "        threshold: Event detection threshold\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (is_event, period_score, summary)\n",
    "    \"\"\"\n",
    "    if len(current_period_data['tweets_edges']) == 0:\n",
    "        return False, -1, \"No tweets found in the current period.\"\n",
    "        \n",
    "    period_score = -1\n",
    "    if previous_periods_data:\n",
    "        # Get weights matrix comparing current period to previous periods\n",
    "        weights = np.zeros((len(current_period_data['vector_edges']), \n",
    "                          len(previous_periods_data)))\n",
    "        \n",
    "        for i, prev_period in enumerate(previous_periods_data):\n",
    "            weights[:, i] = np.asarray(\n",
    "                get_edges_weight(\n",
    "                    prev_period['adjacency_matrix'],\n",
    "                    prev_period['vocabulary'],\n",
    "                    current_period_data['vector_edges'],\n",
    "                    current_period_data['vector_nodes']\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Optimize to get period score\n",
    "        print(f'Optimizing least squares')\n",
    "        period_score = optimize_least_squares(weights, current_period_data['vector'])\n",
    "    \n",
    "    is_event = period_score >= threshold\n",
    "    \n",
    "    return is_event, period_score\n",
    "\n",
    "def optimize_least_squares(A, b):\n",
    "    \"\"\"\n",
    "    Solve the Least Squares optimization problem.\n",
    "    \n",
    "    Args:\n",
    "        A: Weight matrix comparing current period to previous periods\n",
    "        b: Current period vector\n",
    "        \n",
    "    Returns:\n",
    "        Minimum value after optimization\n",
    "    \"\"\"\n",
    "\n",
    "    x = Variable(A.shape[1])\n",
    "    objective = Minimize(norm(A @ x - b))\n",
    "    constraints = [0 <= x, sum(x) == 1]\n",
    "    \n",
    "    minimum = Problem(objective, constraints).solve()\n",
    "    \n",
    "    value = A @ x.value - b\n",
    "    value[value > 0] = 0\n",
    "    minimum = np.linalg.norm(value)\n",
    "    \n",
    "    return minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_window(preprocessed_tweets, vocabulary):\n",
    "    \"\"\"\n",
    "    Process tweets from a single time window.\n",
    "    \n",
    "    Args:\n",
    "        preprocessed_tweets: List of preprocessed tweets\n",
    "        vocabulary: Sorted list of unique words\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing period data\n",
    "    \"\"\"\n",
    "    # Generate adjacency matrix\n",
    "    adj_matrix, tweets_edges = generate_adjacency_matrix_dense(preprocessed_tweets, vocabulary)\n",
    "    \n",
    "    # Generate vector representation\n",
    "    vector, vector_nodes, vector_edges, weighted_edges = generate_vector(adj_matrix, vocabulary)\n",
    "    \n",
    "    return {\n",
    "        'vocabulary': vocabulary,\n",
    "        'adjacency_matrix': adj_matrix,\n",
    "        'tweets_edges': tweets_edges,\n",
    "        'vector': vector,\n",
    "        'vector_nodes': vector_nodes,\n",
    "        'vector_edges': vector_edges,\n",
    "        'weighted_edges': weighted_edges\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_match_file(filename, save_dir = \"preprocessed_data\"):\n",
    "    \n",
    "    base_name = os.path.basename(filename).replace('.csv', '')\n",
    "    save_path = os.path.join(save_dir, f\"{base_name}_preprocessed.pkl\")\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        print(f'Loading existing file at {save_path}')\n",
    "        with open(save_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            df = data['processed_df']\n",
    "            vocabulary = data['vocabulary']\n",
    "    \n",
    "    else :\n",
    "        print(f'No file found, preprocessing...')\n",
    "        df, vocabulary = preprocess_file(filename, save_dir)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='ms')\n",
    "    \n",
    "    # Get ground truth events\n",
    "    ground_truth = (df.groupby(pd.Grouper(key='Timestamp', freq='1min'))['EventType']\n",
    "                   .max()  # If any event in the minute window, count it as event\n",
    "                   .fillna(0)\n",
    "                   .astype(int))\n",
    "    \n",
    "\n",
    "    processed_df = df\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    previous_periods = []\n",
    "    \n",
    "    print(\"Analyzing time windows...\")\n",
    "    # Group by 1-minute windows\n",
    "    for window_start, window_df in processed_df.groupby(\n",
    "        pd.Grouper(key='Timestamp', freq='1min')\n",
    "    ):  \n",
    "        print(f\"Analyzing time at {window_start}, window size {len(window_df)}\")\n",
    "        if len(window_df) == 0:\n",
    "            print(f'No tweets at time {window_start}')\n",
    "            results.append({\n",
    "                'MatchID': df['MatchID'].iloc[0],\n",
    "                'Timestamp': window_start,\n",
    "                'is_event': 0,\n",
    "                'score': 0,\n",
    "                'true_event': ground_truth.get(window_start, 0)\n",
    "            })\n",
    "            continue\n",
    "        # Process current window\n",
    "        print(f'Processing time window')\n",
    "        current_period = process_time_window(window_df, vocabulary)\n",
    "        \n",
    "        # Detect events\n",
    "        print(f'Detecting event')\n",
    "        is_event, score = detect_event(\n",
    "            current_period,\n",
    "            previous_periods,\n",
    "            threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Store results with ground truth\n",
    "        results.append({\n",
    "            'MatchID': df['MatchID'].iloc[0],\n",
    "            'Timestamp': window_start,\n",
    "            'is_event': int(is_event),\n",
    "            'score': score,\n",
    "            'true_event': ground_truth.get(window_start, 0)\n",
    "        })\n",
    "        \n",
    "        # Update previous periods\n",
    "        previous_periods.append(current_period)\n",
    "        if len(previous_periods) > 5:\n",
    "            previous_periods.pop(0)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate classification metrics\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Process all match files in directory\"\"\"\n",
    "    data_dir = \"../challenge_data/train_tweets\"\n",
    "    results = []\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        match_results = process_single_match_file(file_path)\n",
    "        results.append(match_results)\n",
    "        \n",
    "        # Optionally save intermediate results\n",
    "        match_id = match_results['MatchID'].iloc[0]\n",
    "        match_results.to_csv(f'results_match_{match_id}.csv', index=False)\n",
    "    \n",
    "    # Combine all results\n",
    "    final_results = pd.concat(results, ignore_index=True)\n",
    "    final_results = final_results.sort_values(['MatchID', 'Timestamp'])\n",
    "    \n",
    "    # Save final results\n",
    "    final_results.to_csv('all_match_events.csv', index=False)\n",
    "    print(\"\\nAll results saved to all_match_events.csv\")\n",
    "    \n",
    "    return final_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main_train() :\n",
    "    \"\"\"Process all match files and calculate metrics\"\"\"\n",
    "    data_dir = \"../challenge_data/train_tweets\"\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    match_metrics = {}\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        match_results = process_single_match_file(file_path)\n",
    "        \n",
    "        if match_results is not None:\n",
    "            # Calculate metrics for this match\n",
    "            metrics = calculate_metrics(\n",
    "                match_results['true_event'], \n",
    "                match_results['is_event']\n",
    "            )\n",
    "            \n",
    "            match_id = match_results['MatchID'].iloc[0]\n",
    "            match_metrics[match_id] = metrics\n",
    "            \n",
    "            # Add to overall results\n",
    "            all_true.extend(match_results['true_event'])\n",
    "            all_pred.extend(match_results['is_event'])\n",
    "            \n",
    "            print(f\"\\nMetrics for match {match_id}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric}: {value:.3f}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = calculate_metrics(all_true, all_pred)\n",
    "    \n",
    "    print(\"\\nOverall metrics:\")\n",
    "    for metric, value in overall_metrics.items():\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "    \n",
    "    return overall_metrics, match_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing file at preprocessed_data/ArgentinaBelgium72_preprocessed.pkl\n",
      "Analyzing time windows...\n",
      "Analyzing time at 2014-07-05 15:50:00, window size 772\n",
      "Processing time window\n",
      "Generating adjacency matrix\n",
      "Generating vocabulary\n",
      "Getting non zero edges\n",
      "Detecting event\n",
      "Analyzing time at 2014-07-05 15:51:00, window size 724\n",
      "Processing time window\n",
      "Generating adjacency matrix\n",
      "Generating vocabulary\n",
      "Getting non zero edges\n"
     ]
    }
   ],
   "source": [
    "main_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
